kernel = "radial",
cost = 1,
gamma = .05)
plot(svm.radial.smallgamma, nonlinear)
summary(svm.radial.smallgamma)
svm.radial.smallgamma$index
svm.radial.largegamma = svm(y ~ .,
data = nonlinear,
kernel = "radial",
cost = 1,
gamma = 10)
plot(svm.radial.largegamma, nonlinear)
summary(svm.radial.largegamma)
svm.radial.largegamma$index
set.seed(0)
train.index = sample(1:300, 300*.8)
test.index = -train.index
set.seed(0)
cv.svm.radial = tune(svm,
y ~ .,
data = nonlinear[train.index, ],
kernel = "radial",
ranges = list(cost = 10^(seq(-1, 1.5, length = 20)),
gamma = 10^(seq(-2, 1, length = 20))))
#Inspecting the cross-validation output.
summary(cv.svm.radial)
library(rgl)
plot3d(cv.svm.radial$performances$cost,
cv.svm.radial$performances$gamma,
cv.svm.radial$performances$error,
xlab = "Cost",
ylab = "Gamma",
zlab = "Error",
type = "s",
size = 1)
best.nonlinear.model = cv.svm.radial$best.model
summary(best.nonlinear.model)
ypred = predict(best.nonlinear.model, nonlinear[test.index, ])
table("Predicted Values" = ypred, "True Values" = nonlinear[test.index, "y"])
svm.best.nonlinear = svm(y ~ .,
data = nonlinear,
kernel = "radial",
cost = best.nonlinear.model$cost,
gamma = best.nonlinear.model$gamma)
plot(svm.best.nonlinear, nonlinear)
summary(svm.best.nonlinear)
svm.best.nonlinear$index
ypred = predict(svm.best.nonlinear, nonlinear)
table("Predicted Values" = ypred, "True Values" = nonlinear[, "y"])
set.seed(0)
x1 = c(rnorm(100, 2), rnorm(100, -2), rnorm(100), rnorm(100, 2))
x2 = c(rnorm(100, 2), rnorm(100, -2), rnorm(100), rnorm(100, -2))
y = as.factor(c(rep(-1, 200), rep(1, 100), rep(2, 100)))
multi = data.frame(x1, x2, y)
plot(multi$x1, multi$x2, col = multi$y)
set.seed(0)
train.index = sample(1:400, 400*.8)
test.index = -train.index
set.seed(0)
cv.multi = tune(svm,
y ~ .,
data = multi[train.index, ],
kernel = "radial",
ranges = list(cost = 10^(seq(-1, 1.5, length = 20)),
gamma = 10^(seq(-2, 1, length = 20))))
#Inspecting the cross-validation output.
summary(cv.multi)
plot3d(cv.multi$performances$cost,
cv.multi$performances$gamma,
cv.multi$performances$error,
xlab = "Cost",
ylab = "Gamma",
zlab = "Error",
type = "s",
size = 1)
best.multi.model = cv.multi$best.model
summary(best.multi.model)
ypred = predict(best.multi.model, multi[test.index, ])
table("Predicted Values" = ypred, "True Values" = multi[test.index, "y"])
svm.best.multi = svm(y ~ .,
data = multi,
kernel = "radial",
cost = best.multi.model$cost,
gamma = best.multi.model$gamma)
plot(svm.best.multi, multi)
summary(svm.best.multi)
svm.best.multi$index
ypred = predict(svm.best.multi, multi)
table("Predicted Values" = ypred, "True Values" = multi[, "y"])
set.seed(0)
samples=list()
for (i in 1:10000) {
samples[[i]] = runif(100)
}
means = sapply(samples, mean)
hist(unlist(samples))
hist(means)
runif(100)
library(kernlab)
install.packages("kernlab")
library(kernlab)
data(spam)
help(spam)
View(spam)
head(spam)
summary(spam)
library(Hmisc)
aggr(spam)
md.pattern(spam)
library(mice)
aggr(spam)
library(VIM)
aggr(spam)
summary(spam)
corr(spam)
library(corrplot)
corr(spam)
corrplot(spam)
library(ggplot2)
corr(spam)
cor(spam)
cor(spam[,-1])
summary(spam)
dim(spam)
cor(spam[,-58])
corrplot(spam[spam$type=='spam',-58])
spam[spam$type=='spam',-58]
corrplot(cor(spam[spam$type=='spam',-58]))
corrplot(cor(spam[spam$type=='nonspam',-58]))
cor_nonspam = cor(spam[spam$type=='nonspam',-58])
cor_spam = cor(spam[spam$type=='spam',-58])
type(cor_spam)
class(cor_spam)
cor_spam - cor_nonspam
cor_di = cor_spam - cor_nonspam
library(dplyr)
spam %>% group_by(type) %>% summarise(mean)
class(spam)
library(dplyr)
spam %>% group_by(type) %>% summarise(mean)
spam %>% group_by(type)
spam %>% group_by(type) %>% summarise(mean())
?aggregate
aggregate(.~type, spam, mean)
avg = aggregate(.~type, spam, mean)
class(avg)
library(reshape2)
melt(avg)
avg_melted = melt(avg)
avg_melted
ggplot(avg_melted, aes(x=variable)) + geom_bar()
ggplot(avg_melted, aes(x=variable, y=value)) + geom_bar(stat='identity')
ggplot(avg_melted, aes(x=variable, y=value)) + geom_bar(stat='identity', aes(colour=type))
spam_scaled = scale(spam)
avg = aggregate(.~type, spam, mean)
dim(avg)
avg_melted = melt(avg[,c(1:54)])
ggplot(avg_melted, aes(x=variable, y=value)) + geom_bar(stat='identity', aes(colour=type))
avg = aggregate(.~type, spam, mean)
library(reshape2)
class(avg)
dim(avg)
avg_t = t(avg)
avg_t
t(avg)
t(avg[,c(2:58)])
avg_t = t(avg[,c(2:58)])
names(avg_t) = c('spam','nonspam')
avg = aggregate(.~type, spam, mean)
avg
avg_t
avg_t = t(avg[,c(2:58)])
avg_t
avg_t$diff = avg_t[,1]-avg_t[,2]
avg_t
avg_t = t(avg[,c(2:58)])
class(avg_t)
avg_t = as.dataframe(t(avg[,c(2:58)]))
avg_t = as.data.frame(t(avg[,c(2:58)]))
avg_t
names(avg_t) = c('nonspam','spam')
avg_t$diff = avg_t[,1]-avg_t[,2]
avg_t
avg_t[order(avg_t$diff),]
avg_t[order(avg_t$diff),]
avg_t$item = rownames(avg_t)
avg_t
ggplot(avg_t, aes(x=item, y=diff)) + geom_bar(stat='identity')
avg_t$item = rownames(avg_t)
avg_t = as.data.frame(t(avg[,c(2:58)]))
names(avg_t) = c('nonspam','spam')
avg_t$diff = avg_t[,1]-avg_t[,2]
avg_t[order(avg_t$diff),]
avg_t = as.data.frame(t(avg[,c(2:58)]))
names(avg_t) = c('nonspam','spam')
avg_t$diff = abs(avg_t[,1]-avg_t[,2])
avg_t[order(avg_t$diff),]
corrplot(cor(spam))
spam$type <- ifelse(as.character(spam$type)=='spam', 1, 0)
corrplot(cor(spam))
spam$type <- ifelse(as.character(spam$type)=='spam', 1, 0)
corrplot(cor(spam))
data(spam)
spam$type <- ifelse(spam$type=='spam', 1, 0)
corrplot(cor(spam))
cor2 = cor(spam)
class(cor2)
cor2
cor[58,]
cor2[58,]
var = as.data.frame(cor2[58,])
var
names(var) = 'cor'
var
var[order(var$cor),]
var
var$item = rownames(var)
var
var[order(var$cor),]
avg_t = as.data.frame(t(avg[,c(2:58)]))
names(avg_t) = c('nonspam','spam')
avg_t$diff = abs(avg_t[,1]-avg_t[,2])
avg_t[order(avg_t$diff),]
avg_melted = melt(avg[,c(1:54)])
ggplot(avg_melted, aes(x=variable, y=value)) + geom_bar(stat='identity', aes(colour=type))
ggplot(avg_melted, aes(x=variable, y=value)) + geom_bar(stat='identity', aes(colour=type), position="fill")
avg_t
cor_di = cor_spam - cor_nonspam
View(cor_di)
cor_di = round(cor_spam - cor_nonspam)
cor_di = round(cor_spam - cor_nonspam,2)
summary(cor_di)
avg(cor_di)
mean(cor_di)
avg_t = as.data.frame(t(avg[,c(2:58)]))
names(avg_t) = c('nonspam','spam')
avg_t$diff = abs(avg_t[,1]-avg_t[,2])
avg_t[order(avg_t$diff),]
library(pROC)
setwd("~/Desktop/Capstone")
FieldNames <-read.csv("Field Names.csv", header = FALSE,
stringsAsFactors = FALSE)
column.names <- FieldNames[,1] #41 columns
KDD.test <-read.csv("KDDTest+.csv", header = FALSE,
stringsAsFactors = FALSE)
KDD.train <-read.csv("KDDTrain+.csv", header = FALSE,
stringsAsFactors = FALSE)
colnames(KDD.test) <- column.names # rename columns
colnames(KDD.train)<- column.names
#KDD.train$outcome <- as.factor(KDD.train$outcome)
names(KDD.train)[42] <- "outcome"
KDD.train$outcome <- as.factor(KDD.train$outcome)
KDD.train$outcome.response <- ifelse(KDD.train$outcome == 'normal',0,1)
View(KDD.train) #44 cols  0.465% are malicious
View(KDD.test)
#Dealing with 3 Categorical Variables, 0/1, expanding ncols, replace into new.KDD.train
service_<-as.data.frame(class.ind(KDD.train$service))
protocol_type_<-as.data.frame(class.ind(KDD.train$protocol_type))
flag_<-as.data.frame(class.ind(KDD.train$flag))
new <- cbind(service_, protocol_type_, flag_) #84
new.KDD.train <-cbind(duration=KDD.train$duration, new, KDD.train[,5:41], outcome.response=KDD.train[,44])
dim(new.KDD.train) #[1] 125973    123
View(new.KDD.train)
install.packages("nnet")
FieldNames <-read.csv("Field Names.csv", header = FALSE,
stringsAsFactors = FALSE)
column.names <- FieldNames[,1] #41 columns
KDD.test <-read.csv("KDDTest+.csv", header = FALSE,
stringsAsFactors = FALSE)
KDD.train <-read.csv("KDDTrain+.csv", header = FALSE,
stringsAsFactors = FALSE)
colnames(KDD.test) <- column.names # rename columns
colnames(KDD.train)<- column.names
#KDD.train$outcome <- as.factor(KDD.train$outcome)
names(KDD.train)[42] <- "outcome"
KDD.train$outcome <- as.factor(KDD.train$outcome)
KDD.train$outcome.response <- ifelse(KDD.train$outcome == 'normal',0,1)
View(KDD.train) #44 cols  0.465% are malicious
View(KDD.test)
#Dealing with 3 Categorical Variables, 0/1, expanding ncols, replace into new.KDD.train
service_<-as.data.frame(class.ind(KDD.train$service))
protocol_type_<-as.data.frame(class.ind(KDD.train$protocol_type))
flag_<-as.data.frame(class.ind(KDD.train$flag))
new <- cbind(service_, protocol_type_, flag_) #84
new.KDD.train <-cbind(duration=KDD.train$duration, new, KDD.train[,5:41], outcome.response=KDD.train[,44])
dim(new.KDD.train) #[1] 125973    123
View(new.KDD.train)
library(nnet)
FieldNames <-read.csv("Field Names.csv", header = FALSE,
stringsAsFactors = FALSE)
column.names <- FieldNames[,1] #41 columns
KDD.test <-read.csv("KDDTest+.csv", header = FALSE,
stringsAsFactors = FALSE)
KDD.train <-read.csv("KDDTrain+.csv", header = FALSE,
stringsAsFactors = FALSE)
colnames(KDD.test) <- column.names # rename columns
colnames(KDD.train)<- column.names
#KDD.train$outcome <- as.factor(KDD.train$outcome)
names(KDD.train)[42] <- "outcome"
KDD.train$outcome <- as.factor(KDD.train$outcome)
KDD.train$outcome.response <- ifelse(KDD.train$outcome == 'normal',0,1)
View(KDD.train) #44 cols  0.465% are malicious
View(KDD.test)
#Dealing with 3 Categorical Variables, 0/1, expanding ncols, replace into new.KDD.train
service_<-as.data.frame(class.ind(KDD.train$service))
protocol_type_<-as.data.frame(class.ind(KDD.train$protocol_type))
flag_<-as.data.frame(class.ind(KDD.train$flag))
new <- cbind(service_, protocol_type_, flag_) #84
new.KDD.train <-cbind(duration=KDD.train$duration, new, KDD.train[,5:41], outcome.response=KDD.train[,44])
dim(new.KDD.train) #[1] 125973    123
View(new.KDD.train)
View(KDD.train)
str(new.KDD.train)
library(nnet)
FieldNames <-read.csv("Field Names.csv", header = FALSE,
stringsAsFactors = FALSE)
column.names <- FieldNames[,1] #41 columns
KDD.test <-read.csv("KDDTest+.csv", header = FALSE,
stringsAsFactors = FALSE)
KDD.train <-read.csv("KDDTrain+.csv", header = FALSE,
stringsAsFactors = FALSE)
colnames(KDD.test) <- column.names # rename columns
colnames(KDD.train)<- column.names
names(KDD.train)[42] <- "outcome"
KDD.train$outcome <- as.factor(KDD.train$outcome)
KDD.train$outcome.response <- ifelse(KDD.train$outcome == 'normal',0,1)
service_<-as.data.frame(class.ind(KDD.train$service))
protocol_type_<-as.data.frame(class.ind(KDD.train$protocol_type))
flag_<-as.data.frame(class.ind(KDD.train$flag))
new <- cbind(service_, protocol_type_, flag_) #84
new.KDD.train <-cbind(duration=KDD.train$duration, new, KDD.train[,5:41], outcome.response=KDD.train[,44])
dim(new.KDD.train) #[1] 125973    123
View(new.KDD.train)
library(psych)
fa.parallel(new.KDD.train, fa = "pc", n.iter = 100)
abline(h = 1)
View(new.KDD.train)
library(mice)
md.pattern(new.KDD.train)
library(Hmisc)
md.pattern(new.KDD.train)
fa.parallel(new.KDD.train[1:4], fa = "pc", n.iter = 100)
abline(h = 1)
biplot(new.KDD.train)
new.KDD.train[,c(122,123)]
head(new.KDD.train[,c(122,123)])
head(new.KDD.train[,'outcome']
head(new.KDD.train[,'outcome'])
#5
head(new.KDD.train[,'outcome'])
head(new.KDD.train[,outcome])
head(new.KDD.train[,new.KDD.train$outcome])
new.KDD.train$outcome
new.KDD.train[,123]
new.KDD.train[,c(122,123)]
head(new.KDD.train[,c(122,123)])
head(new.KDD.train[,c('outcome','outcome.response')])
head(new.KDD.train[,c('outcome')])
new.KDD.train$outcome
pc_heptathlon = principal(new.KDD.train[,-123], nfactors = 2, rotate = "none")
install.packages("FactoMineR")
library(FactoMineR)
data(decathlon)
head(decathlon)
unique(decathlon$Competition)
res.pca <- PCA(decathlon, quanti.sup = 11:12, quali.sup=13)
barplot(res.pca$eig[,1],main="Eigenvalues",names.arg=1:nrow(res.pca$eig))
library(psych)
fa.parallel(decathlon, fa = "pc", n.iter = 100)
fa.parallel(decathlon[,-13], fa = "pc", n.iter = 100)
abline(h = 1)
res.pca
res.pca$eig
res.pca$eig
?res.pca
?PCA
res.pca <- PCA(decathlon, quanti.sup = 11:12, quali.sup=13, ncp=6)
res.pca$eig
barplot(res.pca$eig[,1],main="Eigenvalues",names.arg=1:nrow(res.pca$eig))
summary(res.pca)
plot(res.pca,choix="ind",habillage=13)
dimdesc(res.pca, axes = 1:2)
plotellipses(res.pca,13)
library(nnet)
FieldNames <-read.csv("Field Names.csv", header = FALSE,
stringsAsFactors = FALSE)
column.names <- FieldNames[,1] #41 columns
KDD.test <-read.csv("KDDTest+.csv", header = FALSE,
stringsAsFactors = FALSE)
KDD.train <-read.csv("KDDTrain+.csv", header = FALSE,
stringsAsFactors = FALSE)
colnames(KDD.test) <- column.names # rename columns
colnames(KDD.train)<- column.names
names(KDD.train)[42] <- "outcome"
KDD.train$outcome <- as.factor(KDD.train$outcome)
KDD.train$outcome.response <- ifelse(KDD.train$outcome == 'normal',0,1)
View(KDD.train) #44 cols  0.465% are malicious
KDD.train2 = KDD.train[,-43]
View(KDD.train2)
str(KDD.train2)
str(KDD.train2[,1:10])
str(KDD.train2[,11:20])
str(KDD.train2[,21:30])
str(KDD.train2[,1:10])
unique(KDD.train2$land)
unique(KDD.train2$wrong_fragment
)
unique(KDD.train2$urgent)
unique(KDD.train2$hot)
str(KDD.train2[,21:30])
str(KDD.train2[,31:40])
str(KDD.train2[,41:43])
KDD.train2 = KDD.train[,-43]
res.pca <- PCA(KDD.train2,
quanti.sup = 43, #Need to add more categorical features
quali.sup=c(2:4,42))
barplot(res.pca$eig[,1],main="Eigenvalues",names.arg=1:nrow(res.pca$eig))
summary(res.pca)
plot(res.pca,choix="ind",habillage=13)
plot(res.pca,choix="ind",habillage=42)
dimdesc(res.pca, axes = 1:2)
plotellipses(res.pca,42)
service_<-as.data.frame(class.ind(KDD.train$service))
protocol_type_<-as.data.frame(class.ind(KDD.train$protocol_type))
flag_<-as.data.frame(class.ind(KDD.train$flag))
new <- cbind(service_, protocol_type_, flag_) #84
new.KDD.train <-cbind(duration=KDD.train$duration, new, KDD.train[,5:41], outcome.response=KDD.train[,44])
dim(new.KDD.train) #[1] 125973    123
View(new.KDD.train)
str(new.KDD.train)
str(new.KDD.train[,100:])
str(new.KDD.train[,100:123])
res.pca <- PCA(new.KDD.train)
KDD.train2 = KDD.train[,-43]
str(KDD.train2)
KDD.train2 = KDD.train[,-c(42,43)]
str(KDD.train2)
index_discrete = c('protocol_type','service','flag','land','logged_in','root_shell','su_attempted','is_hot_login','is_guest_login')
index_discrete = c('protocol_type','service','flag','land','logged_in','root_shell','su_attempted','is_hot_login','is_guest_login','outcome_response')
res.pca <- PCA(KDD.train2,
quali.sup = index_discrete)
library(FactoMineR)
KDD.train2 = KDD.train[,-c(42,43)]
index_discrete = c('protocol_type','service','flag','land','logged_in','root_shell','su_attempted','is_hot_login','is_guest_login','outcome_response')
res.pca <- PCA(KDD.train2,
quali.sup = index_discrete)
str(KDD.train[,1:10])
str(KDD.train[,11:20])
str(KDD.train[,21:30])
str(KDD.train[,31:40])
index_discrete = c(2,3,4,7,12,14,15,21,22,42)
res.pca <- PCA(KDD.train2,
quali.sup = index_discrete)
KDD.train2 = KDD.train[,-c(42,43)]
index_discrete = c(2,3,4,7,12,14,15,21,22,42)
KDD.train2 = KDD.train2[,-index_discrete]
KDD.train2 = KDD.train2[,-13]
library(psych)
fa.parallel(KDD.train2, fa = "pc", n.iter = 100)
abline(h = 1)
pc_KDD.train2 = principal(KDD.train2, nfactors = 9, rotate = "none")
pc_KDD.train2
rm(res.pca)
KDD.train.continuous = KDD.train[,-c(42,43)] #remove outcomes
index_discrete = c(2,3,4,7,12,14,15,21,22,42)
KDD.train.continous = KDD.train.continous[,-c(index_discrete,13)]
KDD.train.continuous = KDD.train.continuous[,-c(index_discrete,13)]
rm(KDD.train2)
rm(pc_KDD.train2)
library(psych)
#Extract only continuous variables
KDD.train.continuous = KDD.train[,-c(42,43)] #remove outcomes
index_discrete = c(2,3,4,7,12,14,15,21,22,42)
#index_discrete = c('protocol_type','service','flag','land','logged_in','root_shell','su_attempted','is_hot_login','is_guest_login','outcome_response')
KDD.train.continuous = KDD.train.continuous[,-c(index_discrete,13)]
#Select number of PCs: 9
fa.parallel(KDD.train.continuous, fa = "pc", n.iter = 100)
abline(h = 1)
#Carry out PCA
pc_KDD.train.continuous = principal(KDD.train.continuous, nfactors = 9, rotate = "none")
pc_KDD.train.continuous
factor.plot(pc_KDD.train.continuous, labels = colnames(KDD.train.continuous))
library(psych)
KDD.train.continuous = KDD.train[,-c(42,43)] #remove outcomes
index_discrete = c(2,3,4,7,12,14,15,21,22,42)
KDD.train.continuous = KDD.train.continuous[,-c(index_discrete,13)]
fa.parallel(KDD.train.continuous, fa = "pc", n.iter = 100)
str(KDD.train.continuous)
KDD.train.continuous = KDD.train[,-c(42,43)] #remove outcomes
index_discrete = c(2,3,4,7,12,14,15,21,22,42)
index_discrete = c(2,3,4,7,12,14,15,21,22,42)
str(KDD.train.continuous)
KDD.train.continuous = KDD.train.continuous[,-c(index_discrete,20)]
fa.parallel(KDD.train.continuous, fa = "pc", n.iter = 100)
abline(h = 1)
#Carry out PCA
pc_KDD.train.continuous = principal(KDD.train.continuous, nfactors = 9, rotate = "none")
pc_KDD.train.continuous
library(psych) #Library that contains helpful PCA functions, such as:
iris_meas = iris[, -5] #Measurements of iris dataset.
iris_meas
plot(iris_meas)
pc_iris = principal(iris_meas, #The data in question.
nfactors = 2,
rotate = "none") #The number of PCs to extract.
pc_iris
factor.plot(pc_iris,
labels = colnames(iris_meas)) #Add variable names to the plot.
plot(iris_meas) #Original data: 4 dimensions.
plot(pc_iris$scores) #Projected data: 2 dimensions.
factor.plot(pc_iris,
labels = colnames(iris_meas)) #Add variable names to the plot.
