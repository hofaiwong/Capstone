(53 + 33)/120
library(MASS)
help(Boston)
set.seed(0)
train = sample(1:nrow(Boston), 7*nrow(Boston)/10)
#Training the tree to predict the median value of owner-occupied homes (in $1k).
tree.boston = tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
#Visually inspecting the regression tree.
plot(tree.boston)
text(tree.boston, pretty = 0)
set.seed(0)
cv.boston = cv.tree(tree.boston)
par(mfrow = c(1, 2))
plot(cv.boston$size, cv.boston$dev, type = "b",
xlab = "Terminal Nodes", ylab = "RSS")
plot(cv.boston$k, cv.boston$dev, type  = "b",
xlab = "Alpha", ylab = "RSS")
?cv.tree
?prune.tree
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations") #Size
plot(cv.carseats$k, cv.carseats$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations") #Complexity
prune.boston = prune.tree(tree.boston, best = 4)
par(mfrow = c(1, 1))
plot(prune.boston)
text(prune.boston, pretty = 0)
yhat = predict(tree.boston, newdata = Boston[-train, ])
yhat
boston.test = Boston[-train, "medv"]
boston.test
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
yhat = predict(prune.boston, newdata = Boston[-train, ])
yhat
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
#Better balance of accuracy and complexity
library(randomForest)
set.seed(0)
rf.boston = randomForest(medv ~ ., data = Boston, subset = train, importance = TRUE)
set.seed(0)
rf.boston = randomForest(medv ~ ., data = Boston, subset = train, importance = TRUE)
rf.boston
set.seed(0)
oob.err = numeric(13)
for (mtry in 1:13) {
fit = randomForest(medv ~ ., data = Boston[train, ], mtry = mtry)
oob.err[mtry] = fit$mse[500] #the function averages as it goes, so the 500th has the average of all 500 MSEs
cat("We're performing iteration", mtry, "\n")
}
#Visualizing the OOB error.
plot(1:13, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
importance(rf.boston)
varImpPlot(rf.boston)
library(gbm)
set.seed(0)
boost.boston = gbm(medv ~ ., data = Boston[train, ],
#=> Shouldn't we substract medv?
distribution = "gaussian", #regression, not classification
n.trees = 10000,
interaction.depth = 4)
par(mfrow = c(1, 1))
summary(boost.boston)
par(mfrow = c(1, 2))
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = Boston[-train, ], n.trees = n.trees)
dim(predmat)
par(mfrow = c(1, 1))
berr = with(Boston[-train, ], apply((predmat - medv)^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
abline(h = min(oob.err), col = "red")
set.seed(0)
boost.boston2 = gbm(medv ~ ., data = Boston[train, ],
#=>Remove medv?
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.1)
predmat2 = predict(boost.boston2, newdata = Boston[-train, ], n.trees = n.trees)
berr2 = with(Boston[-train, ], apply((predmat2 - medv)^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
#Overfitting in play, fitting to training data
#Data scientist art: play with tuning parameters
set.seed(0)
x1 = c(rnorm(100, 0, 4), rnorm(100, 1, 3))
x2 = c(rnorm(100, 0, 1), rnorm(100, 6, 1))
y = as.factor(c(rep(-1, 100), rep(1, 100)))
linearly.separable = data.frame(x1, x2, y)
plot(linearly.separable$x1, linearly.separable$x2, col = linearly.separable$y)
set.seed(0)
train.index = sample(1:200, 200*.8)
test.index = -train.index
library(e1071)
svm.mmc.linear = svm(y ~ ., #Familiar model fitting notation.
data = linearly.separable, #Using the linearly separable data.
subset = train.index, #Using the training data.
kernel = "linear", #Using a linear kernel.
cost = 1e6) #A very large cost; default is 1.
plot(svm.mmc.linear, linearly.separable[train.index, ])
summary(svm.mmc.linear)
svm.mmc.linear$index
ypred = predict(svm.mmc.linear, linearly.separable[test.index, ])
table("Predicted Values" = ypred, "True Values" = linearly.separable[test.index, "y"])
linearly.separable2 = rbind(linearly.separable, c(-5, 3, 1))
plot(linearly.separable2$x1, linearly.separable2$x2, col = linearly.separable2$y)
svm.mmc.linear2 = svm(y ~ .,
data = linearly.separable2,
kernel = "linear",
cost = 1e6)
plot(svm.mmc.linear, linearly.separable[train.index, ]) #Old model.
plot(svm.mmc.linear2, linearly.separable2) #New model.
#Additional information for the fit.
summary(svm.mmc.linear2)
#Finding the indices of the support vectors.
svm.mmc.linear2$index
svm.svc.linear2 = svm(y ~ .,
data = linearly.separable2,
kernel = "linear",
cost = 1)
plot(svm.svc.linear2, linearly.separable2)
summary(svm.svc.linear2)
svm.svc.linear2$index
#What happens if we reduce the cost even more?
svm.svc.linear3 = svm(y ~ .,
data = linearly.separable2,
kernel = "linear",
cost = .1)
plot(svm.svc.linear3, linearly.separable2)
summary(svm.svc.linear3)
svm.svc.linear3$index
set.seed(0)
x1 = c(rnorm(100, -1, 1), rnorm(100, 1, 1))
x2 = c(rnorm(100, -1, 1), rnorm(100, 1, 1))
y = as.factor(c(rep(-1, 100), rep(1, 100)))
overlapping = data.frame(x1, x2, y)
plot(overlapping$x1, overlapping$x2, col = overlapping$y)
set.seed(0)
cv.svm.overlapping = tune(svm,
y ~ .,
data = overlapping[train.index, ],
kernel = "linear",
ranges = list(cost = 10^(seq(-5, .5, length = 100))))
#Inspecting the cross-validation output.
summary(cv.svm.overlapping)
plot(cv.svm.overlapping$performances$cost,
cv.svm.overlapping$performances$error,
xlab = "Cost",
ylab = "Error Rate",
type = "l")
best.overlapping.model = cv.svm.overlapping$best.model
summary(best.overlapping.model)
ypred = predict(best.overlapping.model, overlapping[test.index, ])
table("Predicted Values" = ypred, "True Values" = overlapping[test.index, "y"])
svm.best.overlapping = svm(y ~ .,
data = overlapping,
kernel = "linear",
cost = best.overlapping.model$cost)
plot(svm.best.overlapping, overlapping)
summary(svm.best.overlapping)
svm.best.overlapping$index
ypred = predict(svm.best.overlapping, overlapping)
table("Predicted Values" = ypred, "True Values" = overlapping[, "y"])
set.seed(0)
x1 = c(rnorm(100, 2), rnorm(100, -2), rnorm(100))
x2 = c(rnorm(100, 2), rnorm(100, -2), rnorm(100))
y = as.factor(c(rep(-1, 200), rep(1, 100)))
nonlinear = data.frame(x1, x2, y)
plot(nonlinear$x1, nonlinear$x2, col = nonlinear$y)
svm.radial = svm(y ~ .,
data = nonlinear,
kernel = "radial",
cost = 1,
gamma = .5) #Default is 1/p.
plot(svm.radial, nonlinear)
summary(svm.radial)
svm.radial$index
svm.radial.smallgamma = svm(y ~ .,
data = nonlinear,
kernel = "radial",
cost = 1,
gamma = .05)
plot(svm.radial.smallgamma, nonlinear)
summary(svm.radial.smallgamma)
svm.radial.smallgamma$index
svm.radial.largegamma = svm(y ~ .,
data = nonlinear,
kernel = "radial",
cost = 1,
gamma = 10)
plot(svm.radial.largegamma, nonlinear)
summary(svm.radial.largegamma)
svm.radial.largegamma$index
set.seed(0)
train.index = sample(1:300, 300*.8)
test.index = -train.index
set.seed(0)
cv.svm.radial = tune(svm,
y ~ .,
data = nonlinear[train.index, ],
kernel = "radial",
ranges = list(cost = 10^(seq(-1, 1.5, length = 20)),
gamma = 10^(seq(-2, 1, length = 20))))
#Inspecting the cross-validation output.
summary(cv.svm.radial)
library(rgl)
plot3d(cv.svm.radial$performances$cost,
cv.svm.radial$performances$gamma,
cv.svm.radial$performances$error,
xlab = "Cost",
ylab = "Gamma",
zlab = "Error",
type = "s",
size = 1)
best.nonlinear.model = cv.svm.radial$best.model
summary(best.nonlinear.model)
ypred = predict(best.nonlinear.model, nonlinear[test.index, ])
table("Predicted Values" = ypred, "True Values" = nonlinear[test.index, "y"])
svm.best.nonlinear = svm(y ~ .,
data = nonlinear,
kernel = "radial",
cost = best.nonlinear.model$cost,
gamma = best.nonlinear.model$gamma)
plot(svm.best.nonlinear, nonlinear)
summary(svm.best.nonlinear)
svm.best.nonlinear$index
ypred = predict(svm.best.nonlinear, nonlinear)
table("Predicted Values" = ypred, "True Values" = nonlinear[, "y"])
set.seed(0)
x1 = c(rnorm(100, 2), rnorm(100, -2), rnorm(100), rnorm(100, 2))
x2 = c(rnorm(100, 2), rnorm(100, -2), rnorm(100), rnorm(100, -2))
y = as.factor(c(rep(-1, 200), rep(1, 100), rep(2, 100)))
multi = data.frame(x1, x2, y)
plot(multi$x1, multi$x2, col = multi$y)
set.seed(0)
train.index = sample(1:400, 400*.8)
test.index = -train.index
set.seed(0)
cv.multi = tune(svm,
y ~ .,
data = multi[train.index, ],
kernel = "radial",
ranges = list(cost = 10^(seq(-1, 1.5, length = 20)),
gamma = 10^(seq(-2, 1, length = 20))))
#Inspecting the cross-validation output.
summary(cv.multi)
plot3d(cv.multi$performances$cost,
cv.multi$performances$gamma,
cv.multi$performances$error,
xlab = "Cost",
ylab = "Gamma",
zlab = "Error",
type = "s",
size = 1)
best.multi.model = cv.multi$best.model
summary(best.multi.model)
ypred = predict(best.multi.model, multi[test.index, ])
table("Predicted Values" = ypred, "True Values" = multi[test.index, "y"])
svm.best.multi = svm(y ~ .,
data = multi,
kernel = "radial",
cost = best.multi.model$cost,
gamma = best.multi.model$gamma)
plot(svm.best.multi, multi)
summary(svm.best.multi)
svm.best.multi$index
ypred = predict(svm.best.multi, multi)
table("Predicted Values" = ypred, "True Values" = multi[, "y"])
set.seed(0)
samples=list()
for (i in 1:10000) {
samples[[i]] = runif(100)
}
means = sapply(samples, mean)
hist(unlist(samples))
hist(means)
runif(100)
library(kernlab)
install.packages("kernlab")
library(kernlab)
data(spam)
help(spam)
View(spam)
head(spam)
summary(spam)
library(Hmisc)
aggr(spam)
md.pattern(spam)
library(mice)
aggr(spam)
library(VIM)
aggr(spam)
summary(spam)
corr(spam)
library(corrplot)
corr(spam)
corrplot(spam)
library(ggplot2)
corr(spam)
cor(spam)
cor(spam[,-1])
summary(spam)
dim(spam)
cor(spam[,-58])
corrplot(spam[spam$type=='spam',-58])
spam[spam$type=='spam',-58]
corrplot(cor(spam[spam$type=='spam',-58]))
corrplot(cor(spam[spam$type=='nonspam',-58]))
cor_nonspam = cor(spam[spam$type=='nonspam',-58])
cor_spam = cor(spam[spam$type=='spam',-58])
type(cor_spam)
class(cor_spam)
cor_spam - cor_nonspam
cor_di = cor_spam - cor_nonspam
library(dplyr)
spam %>% group_by(type) %>% summarise(mean)
class(spam)
library(dplyr)
spam %>% group_by(type) %>% summarise(mean)
spam %>% group_by(type)
spam %>% group_by(type) %>% summarise(mean())
?aggregate
aggregate(.~type, spam, mean)
avg = aggregate(.~type, spam, mean)
class(avg)
library(reshape2)
melt(avg)
avg_melted = melt(avg)
avg_melted
ggplot(avg_melted, aes(x=variable)) + geom_bar()
ggplot(avg_melted, aes(x=variable, y=value)) + geom_bar(stat='identity')
ggplot(avg_melted, aes(x=variable, y=value)) + geom_bar(stat='identity', aes(colour=type))
spam_scaled = scale(spam)
avg = aggregate(.~type, spam, mean)
dim(avg)
avg_melted = melt(avg[,c(1:54)])
ggplot(avg_melted, aes(x=variable, y=value)) + geom_bar(stat='identity', aes(colour=type))
avg = aggregate(.~type, spam, mean)
library(reshape2)
class(avg)
dim(avg)
avg_t = t(avg)
avg_t
t(avg)
t(avg[,c(2:58)])
avg_t = t(avg[,c(2:58)])
names(avg_t) = c('spam','nonspam')
avg = aggregate(.~type, spam, mean)
avg
avg_t
avg_t = t(avg[,c(2:58)])
avg_t
avg_t$diff = avg_t[,1]-avg_t[,2]
avg_t
avg_t = t(avg[,c(2:58)])
class(avg_t)
avg_t = as.dataframe(t(avg[,c(2:58)]))
avg_t = as.data.frame(t(avg[,c(2:58)]))
avg_t
names(avg_t) = c('nonspam','spam')
avg_t$diff = avg_t[,1]-avg_t[,2]
avg_t
avg_t[order(avg_t$diff),]
avg_t[order(avg_t$diff),]
avg_t$item = rownames(avg_t)
avg_t
ggplot(avg_t, aes(x=item, y=diff)) + geom_bar(stat='identity')
avg_t$item = rownames(avg_t)
avg_t = as.data.frame(t(avg[,c(2:58)]))
names(avg_t) = c('nonspam','spam')
avg_t$diff = avg_t[,1]-avg_t[,2]
avg_t[order(avg_t$diff),]
avg_t = as.data.frame(t(avg[,c(2:58)]))
names(avg_t) = c('nonspam','spam')
avg_t$diff = abs(avg_t[,1]-avg_t[,2])
avg_t[order(avg_t$diff),]
corrplot(cor(spam))
spam$type <- ifelse(as.character(spam$type)=='spam', 1, 0)
corrplot(cor(spam))
spam$type <- ifelse(as.character(spam$type)=='spam', 1, 0)
corrplot(cor(spam))
data(spam)
spam$type <- ifelse(spam$type=='spam', 1, 0)
corrplot(cor(spam))
cor2 = cor(spam)
class(cor2)
cor2
cor[58,]
cor2[58,]
var = as.data.frame(cor2[58,])
var
names(var) = 'cor'
var
var[order(var$cor),]
var
var$item = rownames(var)
var
var[order(var$cor),]
avg_t = as.data.frame(t(avg[,c(2:58)]))
names(avg_t) = c('nonspam','spam')
avg_t$diff = abs(avg_t[,1]-avg_t[,2])
avg_t[order(avg_t$diff),]
avg_melted = melt(avg[,c(1:54)])
ggplot(avg_melted, aes(x=variable, y=value)) + geom_bar(stat='identity', aes(colour=type))
ggplot(avg_melted, aes(x=variable, y=value)) + geom_bar(stat='identity', aes(colour=type), position="fill")
avg_t
cor_di = cor_spam - cor_nonspam
View(cor_di)
cor_di = round(cor_spam - cor_nonspam)
cor_di = round(cor_spam - cor_nonspam,2)
summary(cor_di)
avg(cor_di)
mean(cor_di)
avg_t = as.data.frame(t(avg[,c(2:58)]))
names(avg_t) = c('nonspam','spam')
avg_t$diff = abs(avg_t[,1]-avg_t[,2])
avg_t[order(avg_t$diff),]
library(pROC)
setwd("~/Library/Containers/com.apple.mail/Data/Library/Mail Downloads/05254E41-B2EC-4827-B6FE-C8E984EFFF64")
#EDA
#logistic
#Ridge
#K-means clustering
setwd("~/Documents/Network_Fraud/")
FieldNames <-read.csv("Field Names.csv", header = FALSE,
stringsAsFactors = FALSE)
KDD.test <-read.csv("KDDTest+.csv", header = FALSE,
stringsAsFactors = FALSE)
KDD.train <-read.csv("KDDTrain+.csv", header = FALSE,
stringsAsFactors = FALSE)
column.names <- FieldNames[,1] #41 columns
colnames(KDD.test) <- column.names # rename columns
colnames(KDD.train)<- column.names
colnames(KDD.train)[42] <- 'outcome'
KDD.train$outcome <- as.factor(KDD.train$outcome)
KDD.train$outcome.response <- ifelse(KDD.train$outcome == 'normal',0,1)
View(KDD.train) #44 cols  0.465% are malicious
View(KDD.test)
#Dealing with 3 Categorical Variables, 0/1, expanding ncols, replace into new.KDD.train
library(nnet)
service_<-as.data.frame(class.ind(KDD.train$service))
protocol_type_<-as.data.frame(class.ind(KDD.train$protocol_type))
flag_<-as.data.frame(class.ind(KDD.train$flag))
new_ <- cbind(service_, protocol_type_, flag_) #84
new.KDD.train <-cbind(duration=KDD.train$duration, new_, KDD.train[,5:41], outcome.response=KDD.train[,44])
dim(new.KDD.train) #[1] 125973    123
View(new.KDD.train)
setwd("~/Library/Containers/com.apple.mail/Data/Library/Mail Downloads/05254E41-B2EC-4827-B6FE-C8E984EFFF64")
FieldNames <-read.csv("Field Names.csv", header = FALSE,
stringsAsFactors = FALSE)
KDD.test <-read.csv("KDDTest+.csv", header = FALSE,
stringsAsFactors = FALSE)
KDD.train <-read.csv("KDDTrain+.csv", header = FALSE,
stringsAsFactors = FALSE)
column.names <- FieldNames[,1] #41 columns
colnames(KDD.test) <- column.names # rename columns
colnames(KDD.train)<- column.names
colnames(KDD.train)[42] <- 'outcome'
KDD.train$outcome <- as.factor(KDD.train$outcome)
KDD.train$outcome.response <- ifelse(KDD.train$outcome == 'normal',0,1)
View(KDD.train) #44 cols  0.465% are malicious
View(KDD.test)
#Dealing with 3 Categorical Variables, 0/1, expanding ncols, replace into new.KDD.train
library(nnet)
service_<-as.data.frame(class.ind(KDD.train$service))
protocol_type_<-as.data.frame(class.ind(KDD.train$protocol_type))
flag_<-as.data.frame(class.ind(KDD.train$flag))
new_ <- cbind(service_, protocol_type_, flag_) #84
new.KDD.train <-cbind(duration=KDD.train$duration, new_, KDD.train[,5:41], outcome.response=KDD.train[,44])
dim(new.KDD.train) #[1] 125973    123
View(new.KDD.train)
FieldNames <-read.csv("Field Names.csv", header = FALSE,
stringsAsFactors = FALSE)
column.names <- FieldNames[,1] #41 columns
colnames(KDD.test) <- column.names # rename columns
colnames(KDD.train)<- column.names
setwd("~/Desktop/Capstone")
FieldNames <-read.csv("Field Names.csv", header = FALSE,
stringsAsFactors = FALSE)
column.names <- FieldNames[,1] #41 columns
colnames(KDD.test) <- column.names # rename columns
colnames(KDD.train)<- column.names
FieldNames <-read.csv("Field Names.csv", header = FALSE,
stringsAsFactors = FALSE)
column.names <- FieldNames[,1] #41 columns
KDD.test <-read.csv("KDDTest+.csv", header = FALSE,
stringsAsFactors = FALSE)
KDD.train <-read.csv("KDDTrain+.csv", header = FALSE,
stringsAsFactors = FALSE)
colnames(KDD.test) <- column.names # rename columns
colnames(KDD.train)<- column.names
names(KDD.train)[42] <- "outcome"
KDD.train$outcome <- as.factor(KDD.train$outcome)
KDD.train$outcome.response <- ifelse(KDD.train$outcome == 'normal',0,1)
View(KDD.train) #44 cols  0.465% are malicious
View(KDD.train)
str(KDD.train)
